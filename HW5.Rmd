---
title: 'Machine Learning 2019: Tree-Based Methods'
author: "Sonali Narang"
date: "10/28/2019"
output:
  html_document:
    df_print: paged
  pdf: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(ISLR)
library(tree)
library(randomForest)
library(MASS)
library(gbm)

```

## Homework

1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results. 
```{r}
library(mlbench)
library(rpart)

data(Glass)
```

```{r}

# Regression Tree Example

# grow tree 
fit <- rpart(RI~Na + Mg + Ca +Si + Type, 
   method="anova", data=Glass)

printcp(fit) # display the results 
plotcp(fit) # plot cross-val results 
summary(fit) # detailed summary of splits

# create additional plots 
par(mfrow=c(1,2)) # two plots on one page 
rsq.rpart(fit) # plot cross-val results   

# plot tree 
plot(fit, uniform=TRUE, 
   main="Regression Tree for Refractive Index")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

pfit<- prune(fit, cp=0.01050071) # from cptable lowest error result

# plot the pruned tree 
plot(pfit, uniform=TRUE, 
   main="Pruned Regression Tree for Refractive Index")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```



I used recursive partitioning for my regression tree-based method. The results suggest that of the provided variables, Calcium has the strongest importance to the model. Pruning the tree using the complexity cost factor at the number of splits associated with the the lowest error naturally resulting in a simpler tree, suggesting 8 rather than 9 is the optimal number of splits. 

2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.

```{r}
data(Glass)
```

```{r}
#Bagging on Glass dataset
#set seed for reproducibility 
set.seed(29)

#split into train and test sets (180 and 34 respectively)
train = sample(1:nrow(Glass), 180)

#fit training subset of data to model 
rf.glass = randomForest(RI~., data = Glass, subset = train)
rf.glass

#summary of rf.boston gives information about the number of trees, the mean squared residuals (MSR), and the percentage of variance explained

#No. of variables tried at each split: 3

oob.err = double(8)
test.err = double(8)

#use 1-8 as 8 was the lowest error of the last model
for(mtry in 1:8){
  fit = randomForest(RI~., data = Glass, subset=train, mtry=mtry, ntree = 350)
  oob.err[mtry] = fit$mse[350] ##extract Mean-squared-error 
  pred = predict(fit, Glass[-train,]) #predict on test dataset
  test.err[mtry] = with(Glass[-train,], mean( (RI-pred)^2 )) #compute test error
}

#Visualize 
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))

```
Using Out of Bag for the random forest bagging method, I find that the optimal number of splits is 7, as there is the least amount of overfitting there according to the mean square error.

```{r}
#Gradient Boosting Model
#Bagging on Glass dataset
#set seed for reproducibility 
set.seed(29)

#split into train and test sets (half and half)
train = sample(1:nrow(Glass), 107)

boost.Glass = gbm(RI~., data = Glass[train,], distribution = "gaussian", n.trees = 1000, shrinkage = 0.01, interaction.depth = 4)

#Variable Importance Plot
summary(boost.Glass)
help(summary)
#Visualize important variables of interest
plot(boost.Glass,i="Ca")
plot(boost.Glass,i="Si")

#Predict on test set
n.trees = seq(from = 100, to = 2000, by = 50)
predmat = predict(boost.Glass, newdata = Glass[-train,], n.trees = n.trees)
dim(predmat)

#Visualize Boosting Error Plot
boost.err = with(Glass[-train,], apply( (predmat - RI)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")

```

It appears that boosting is an effective method, as the test error is below 2.6e-06 at only a few hundred trees and reaches its nadir at around 100 trees. Further, it seems that influential variables of Ca and Si plateau as expected.